@inproceedings{mask-rcnn,
author = {He, K and Gkioxari, G and Doll{\'{a}}r, P and Girshick, R},
booktitle = {Proc. of ICCV},
pages = {2980--2988},
title = {Mask R-CNN},
year = {2017}
}
@inproceedings{qian2019learning,
archivePrefix = {arXiv},
arxivId = {cs.CV/1911.08299},
author = {Qian, Wen and Yang, Xue and Peng, Silong and Guo, Yue and Yan, Chijun and Yan, Junchi},
booktitle = {Proc. of AAAI},
eprint = {1911.08299},
primaryClass = {cs.CV},
title = {Learning Modulated Loss for Rotated Object Detection},
year = {2019}
}
@article{han2020align,
archivePrefix = {arXiv},
arxivId = {cs.CV/2008.09397},
author = {Han, Jiaming and Ding, Jian and Li, Jie and Xia, Gui-Song},
doi = {10.1109/TGRS.2021.3062048},
eprint = {2008.09397},
journal = {IEEE Trans. on Geoscience and Remote Sensing},
primaryClass = {cs.CV},
title = {Align Deep Features for Oriented Object Detection},
year = {2021}
}
@inproceedings{yolo,
author = {Redmon, J and Divvala, S and Girshick, R and Farhadi, A},
booktitle = {Proc. of CVPR},
title = {You Only Look Once: Unified, Real-Time Object Detection},
year = {2016}
}
@misc{sun2018salience,
archivePrefix = {arXiv},
arxivId = {cs.CV/1810.08103},
author = {Sun, Peng and Chen, Guang and Luke, Guerdan and Shang, Yi},
eprint = {1810.08103},
primaryClass = {cs.CV},
title = {Salience Biased Loss for Object Detection in Aerial Images},
year = {2018}
}
@misc{zhou2020objects,
archivePrefix = {arXiv},
arxivId = {cs.CV/2001.02988},
author = {Zhou, Lin and Wei, Haoran and Li, Hao and Zhao, Wenzhe and Zhang, Yi and Zhang, Yue},
eprint = {2001.02988},
primaryClass = {cs.CV},
title = {Objects detection for remote sensing images based on polar coordinates},
year = {2020}
}
@inproceedings{fcos,
author = {Tian, Zhi and Shen, Chunhua and Chen, Hao and He, Tong},
booktitle = {Proc. of ICCV},
title = {FCOS: Fully Convolutional One-Stage Object Detection},
year = {2019}
}
@misc{goyal2017accurateLM,
archivePrefix = {arXiv},
arxivId = {cs.CV/1706.02677},
author = {Goyal, Priya and Doll{\'{a}}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
eprint = {1706.02677},
primaryClass = {cs.CV},
title = {Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour},
year = {2018}
}
@inproceedings{unitbox,
address = {New York, NY, USA},
author = {Yu, Jiahui and Jiang, Yuning and Wang, Zhangyang and Cao, Zhimin and Huang, Thomas},
booktitle = {Proc. of ACM International Conference on Multimedia},
doi = {10.1145/2964284.2967274},
isbn = {9781450336031},
keywords = {IoU loss,bounding box prediction,object detection},
publisher = {Association for Computing Machinery},
series = {MM '16},
title = {UnitBox: An Advanced Object Detection Network},
year = {2016}
}
@inproceedings{yolo-v2,
author = {Redmon, J and Farhadi, A},
booktitle = {Proc. of CVPR},
title = {YOLO9000: Better, Faster, Stronger},
year = {2017}
}
@inproceedings{roi-trans,
author = {Ding, J and Xue, N and Long, Y and Xia, G and Lu, Q},
booktitle = {Proc. of CVPR},
title = {Learning RoI Transformer for Oriented Object Detection in Aerial Images},
year = {2019}
}
@misc{od-20-years,
archivePrefix = {arXiv},
arxivId = {cs.CV/1905.05055},
author = {Zou, Zhengxia and Shi, Zhenwei and Guo, Yuhong and Ye, Jieping},
eprint = {1905.05055},
primaryClass = {cs.CV},
title = {Object Detection in 20 Years: A Survey},
year = {2019}
}
@inproceedings{pmlr-v37-xuc15,
abstract = {Inspired by recent work in machine translation and object
detection, we introduce an attention based model that
automatically learns to describe the content of images. We
describe how we can train this model in a deterministic manner
using standard backpropagation techniques and stochastically
by maximizing a variational lower bound. We also show through
visualization how the model is able to automatically learn to
fix its gaze on salient objects while generating the
corresponding words in the output sequence. We validate the
use of attention with state-of-the-art performance on three
benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
address = {Lille, France},
author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},
booktitle = {Proc. of ICML},
editor = {Bach, Francis and Blei, David},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {Show, Attend and Tell: Neural Image Caption Generation with Visual Attention},
volume = {37},
year = {2015}
}
@article{zhou2020polar,
author = {Zhou, L and Wei, H and Li, H and Zhao, W and Zhang, Y and Zhang, Y},
doi = {10.1109/ACCESS.2020.3041025},
journal = {IEEE Access},
title = {Arbitrary-Oriented Object Detection in Remote Sensing Images Based on Polar Coordinates},
year = {2020}
}
@inproceedings{fpns,
author = {Lin, T and Doll{\'{a}}r, P and Girshick, R and He, K and Hariharan, B and Belongie, S},
booktitle = {Proc. of CVPR},
title = {Feature Pyramid Networks for Object Detection},
year = {2017}
}
@inproceedings{ssd,
author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott E and Fu, Cheng-Yang and Berg, Alexander C},
booktitle = {Proc. of ECCV},
title = {SSD: Single Shot MultiBox Detector},
year = {2016}
}
@inproceedings{imagenet_cvpr09,
author = {Deng, J and Dong, W and Socher, R and Li, L.-J. and Li, K and Fei-Fei, L},
booktitle = {Proc. of CVPR},
title = {ImageNet: A Large-Scale Hierarchical Image Database},
year = {2009}
}
@misc{yolo-v3,
archivePrefix = {arXiv},
arxivId = {cs.CV/1804.02767},
author = {Redmon, Joseph and Farhadi, Ali},
eprint = {1804.02767},
primaryClass = {cs.CV},
title = {YOLOv3: An Incremental Improvement},
year = {2018}
}
@inproceedings{yang2021rethinking,
author = {Yang, X and Yan, Junchi and Ming, Qi and Wang, Wentao and Zhang, Xiaopeng and Tian, Qi},
booktitle = {Proc. of ICML},
title = {Rethinking Rotated Object Detection with Gaussian Wasserstein Distance Loss},
year = {2021}
}
@book{goodfellow2016deeplearning,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
isbn = {0262035618},
publisher = {The MIT Press},
title = {Deep Learning},
year = {2016}
}
@inproceedings{orn,
author = {Zhou, Y and Ye, Q and Qiu, Q and Jiao, J},
booktitle = {Proc. of CVPR},
doi = {10.1109/CVPR.2017.527},
title = {Oriented Response Networks},
year = {2017}
}
@inproceedings{resnet,
author = {He, K and Zhang, X and Ren, S and Sun, J},
booktitle = {Proc. of CVPR},
title = {Deep Residual Learning for Image Recognition},
year = {2016}
}
@inproceedings{faster-rcnn,
address = {Cambridge, MA, USA},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
booktitle = {Proc. of NeurIPS},
publisher = {MIT Press},
series = {NIPS'15},
title = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
year = {2015}
}
@inproceedings{Li_2019_CVPR_Workshops,
author = {Li, Chengzheng and Xu, Chunyan and Cui, Zhen and Wang, Dan and Jie, Zequn and Zhang, Tong and Yang, Jian},
booktitle = {CVPR Workshops},
month = {jun},
title = {Learning Object-Wise Semantic Representation for Detection in Remote Sensing Imagery},
year = {2019}
}
@article{xiao2020axis,
author = {Xiao, Zhifeng and Qian, Linjun and Shao, Weiping and Tan, Xiaowei and Wang, Kai},
doi = {10.3390/rs12060908},
issn = {2072-4292},
journal = {Remote Sensing},
month = {mar},
number = {6},
publisher = {MDPI AG},
title = {Axis Learning for Orientated Objects Detection in Aerial Images},
volume = {12},
year = {2020}
}
@misc{yang2020scrdet,
archivePrefix = {arXiv},
arxivId = {cs.CV/2004.13316},
author = {Yang, Xue and Yan, Junchi and Yang, Xiaokang and Tang, Jin and Liao, Wenlong and He, Tao},
eprint = {2004.13316},
primaryClass = {cs.CV},
title = {SCRDet++: Detecting Small, Cluttered and Rotated Objects via Instance-Level Feature Denoising and Rotation Loss Smoothing},
year = {2020}
}
@inproceedings{chen2020piou,
author = {Chen, Zhiming and Chen, Ke-An and Lin, Weiyao and See, John and Yu, Hui and Ke, Yan and Yang, Cong},
booktitle = {Proc. of ECCV},
title = {PIoU Loss: Towards Accurate Oriented Object Detection in Complex Environments},
year = {2020}
}
@article{8003302,
author = {Kang, K and Li, H and Yan, J and Zeng, X and Yang, B and Xiao, T and Zhang, C and Wang, Z and Wang, R and Wang, X and Ouyang, W},
journal = {IEEE Trans. on Circuits and Systems for Video Technology},
number = {10},
title = {T-CNN: Tubelets With Convolutional Neural Networks for Object Detection From Videos},
volume = {28},
year = {2018}
}
@article{Wu2018ImageCA,
author = {Wu, Qi and Shen, Chunhua and Wang, Peng and Dick, Anthony and van den Hengel, Anton},
journal = {IEEE Trans. on Pattern Analysis and Machine Intelligence},
title = {Image Captioning and Visual Question Answering Based on Attributes and External Knowledge},
volume = {40},
year = {2018}
}
@inproceedings{Xia_2018_CVPR,
author = {Xia, Gui-Song and Bai, Xiang and Ding, Jian and Zhu, Zhen and Belongie, Serge and Luo, Jiebo and Datcu, Mihai and Pelillo, Marcello and Zhang, Liangpei},
booktitle = {Proc. of CVPR},
month = {jun},
title = {DOTA: A Large-Scale Dataset for Object Detection in Aerial Images},
year = {2018}
}
@article{yang2020arbitrary,
author = {Yang, Xue and Yan, Junchi},
institution = {Springer},
journal = {Proc. of ECCV},
title = {Arbitrary-Oriented Object Detection with Circular Smooth Label},
year = {2020}
}
@misc{lin2019ienet,
archivePrefix = {arXiv},
arxivId = {cs.CV/1912.00969},
author = {Lin, Youtian and Feng, Pengming and Guan, Jian},
eprint = {1912.00969},
primaryClass = {cs.CV},
title = {IENet: Interacting Embranchment One Stage Anchor Free Detector for Orientation Aerial Object Detection},
year = {2019}
}
@article{jia2021af-ems,
author = {Yan, Jiangqiao and Zhao, Liangjin and Diao, Wenhui and Wang, Hongqi and Sun, Xian},
doi = {10.3390/rs13020160},
issn = {2072-4292},
journal = {Remote Sensing},
number = {2},
title = {AF-EMS Detector: Improve the Multi-Scale Detection Performance of the Anchor-Free Detector},
volume = {13},
year = {2021}
}
@inproceedings{hypercolumns,
author = {Hariharan, B and Arbel{\'{a}}ez, P and Girshick, R and Malik, J},
booktitle = {Proc. of CVPR},
title = {Hypercolumns for object segmentation and fine-grained localization},
year = {2015}
}
@article{wei2020o2-dnet,
author = {Wei, Haoran and Zhang, Yue and Chang, Zhonghan and Li, Hao and Wang, Hongqi and Sun, Xian},
doi = {https://doi.org/10.1016/j.isprsjprs.2020.09.022},
issn = {0924-2716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Anchor-free,Middle lines,NMS-free,Object detection,Oriented objects},
title = {Oriented objects as pairs of Middle Lines},
volume = {169},
year = {2020}
}
@inproceedings{simul-det-and-seg,
address = {Cham},
author = {Hariharan, Bharath and Arbel{\'{a}}ez, Pablo and Girshick, Ross and Malik, Jitendra},
booktitle = {Proc. of ECCV},
editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
isbn = {978-3-319-10584-0},
publisher = {Springer International Publishing},
title = {Simultaneous Detection and Segmentation},
year = {2014}
}
@inproceedings{centernet,
author = {Duan, K and Bai, S and Xie, L and Qi, H and Huang, Q and Tian, Q},
booktitle = {Proc. of ICCV},
title = {CenterNet: Keypoint Triplets for Object Detection},
year = {2019}
}
@inproceedings{deform-conv,
author = {Dai, J and Qi, H and Xiong, Y and Li, Y and Zhang, G and Hu, H and Wei, Y},
booktitle = {Proc. of ICCV},
doi = {10.1109/ICCV.2017.89},
title = {Deformable Convolutional Networks},
year = {2017}
}
@article{hou2021sara,
author = {Hou, Jie-Bo and Zhu, Xiaobin and Yin, Xu-Cheng},
doi = {10.3390/rs13071318},
issn = {2072-4292},
journal = {Remote Sensing},
number = {7},
title = {Self-Adaptive Aspect Ratio Anchor for Oriented Object Detection in Remote Sensing Images},
volume = {13},
year = {2021}
}
@misc{kong2019foveabox,
archivePrefix = {arXiv},
arxivId = {cs.CV/1904.03797},
author = {Kong, Tao and Sun, Fuchun and Liu, Huaping and Jiang, Yuning and Shi, Jianbo},
eprint = {1904.03797},
primaryClass = {cs.CV},
title = {FoveaBox: Beyond Anchor-based Object Detector},
year = {2019}
}
@article{cornernet,
author = {Law, Hei and Deng, Jia},
doi = {10.1007/s11263-019-01204-1},
journal = {International Journal of Computer Vision},
title = {CornerNet: Detecting Objects as Paired Keypoints},
year = {2019}
}
@misc{Chen2017RethinkingAC,
archivePrefix = {arXiv},
arxivId = {cs.CV/1706.05587},
author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
eprint = {1706.05587},
primaryClass = {cs.CV},
title = {Rethinking Atrous Convolution for Semantic Image Segmentation},
year = {2017}
}
@misc{Liu2017LearningAR,
archivePrefix = {arXiv},
arxivId = {cs.CV/1711.09405},
author = {Liu, Lei and Pan, Zongxu and Lei, Bin},
eprint = {1711.09405},
primaryClass = {cs.CV},
title = {Learning a Rotation Invariant Detector with Rotatable Bounding Box},
year = {2017}
}
@inproceedings{extremenet,
author = {Zhou, Xingyi and Zhuo, Jiacheng and Kr{\"{a}}henb{\"{u}}hl, Philipp},
booktitle = {Proc. of CVPR},
title = {Bottom-up Object Detection by Grouping Extreme and Center Points},
year = {2019}
}
@article{fu2021frest,
author = {Fu, K and Chang, Z and Zhang, Y and Sun, X},
doi = {10.1109/TGRS.2020.3020165},
journal = {IEEE Trans. on Geoscience and Remote Sensing},
title = {Point-Based Estimator for Arbitrary-Oriented Object Detection in Aerial Images},
year = {2020}
}
@article{Jianqi17RRPN,
author = {Ma, Jianqi and Shao, Weiyuan and Ye, Hao and Wang, Li and Wang, Hong and Zheng, Yingbin and Xue, Xiangyang},
journal = {IEEE Trans. on Multimedia},
number = {11},
title = {Arbitrary-Oriented Scene Text Detection via Rotation Proposals},
volume = {20},
year = {2018}
}
@inproceedings{karpathy2015deep,
author = {Karpathy, Andrej and Fei-Fei, Li},
booktitle = {Proc. of CVPR},
title = {Deep visual-semantic alignments for generating image descriptions},
year = {2015}
}
@inproceedings{hrsc2016,
author = {Liu, Zikun and Yuan, Liu and Weng, Lubin and Yang, Yiping},
doi = {10.5220/0006120603240331},
title = {A High Resolution Optical Satellite Image Dataset for Ship Recognition and Some New Baselines},
year = {2017}
}
@article{retinanet,
author = {Lin, T and Goyal, P and Girshick, R and He, K and Doll{\'{a}}r, P},
journal = {IEEE Trans. on Pattern Analysis and Machine Intelligence},
number = {2},
title = {Focal Loss for Dense Object Detection},
volume = {42},
year = {2020}
}
@article{instance-aware-semantiv-seg,
author = {Dai, Jifeng and He, Kaiming and Sun, Jian},
journal = {Proc. of CVPR},
title = {Instance-Aware Semantic Segmentation via Multi-task Network Cascades},
year = {2016}
}
@article{hinton2014dropout,
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
journal = {Journal of Machine Learning Research},
number = {56},
pages = {1929--1958},
title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
volume = {15},
year = {2014}
}
@article{ILSVRC15,
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
journal = {International Journal of Computer Vision (IJCV)},
number = {3},
pages = {211--252},
title = {ImageNet Large Scale Visual Recognition Challenge},
volume = {115},
year = {2015}
}
@article{liao2018textboxes,
author = {Liao, M and Shi, B and Bai, X},
doi = {10.1109/TIP.2018.2825107},
journal = {IEEE Transactions on Image Processing},
number = {8},
pages = {3676--3690},
title = {TextBoxes++: A Single-Shot Oriented Scene Text Detector},
volume = {27},
year = {2018}
}
@inproceedings{liu2019mnidirectionalST,
author = {Liu, Y and Zhang, Sheng and Jin, Lianwen and Xie, Lele and Wu, Y and Wang, Z},
booktitle = {IJCAI},
title = {Omnidirectional Scene Text Detection with Sequential-free Box Discretization},
year = {2019}
}
@article{Goodfellow2014,
abstract = {We propose a new framework for estimating generative models via an adversar- ial process; in which we simultaneously train two models: a generative model G that captures the data distribution; and a discriminative model D that estimates the probability that a sample came from the training data rather thanG. The train- ing procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D; a unique solution exists; with G recovering the training data distribution andD equal to 1 2 everywhere. In the case where G andD are defined by multilayer perceptrons; the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference net- works during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples. 1},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.2661v1},
author = {Goodfellow, Ian J and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
eprint = {arXiv:1406.2661v1},
journal = {Neural Information Processing Systems (NeurIPS)},
title = {Generative Adversarial Nets},
year = {2014}
}
@misc{simonyan2016vgg,
archivePrefix = {arXiv},
arxivId = {cs.CV/1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {1409.1556},
primaryClass = {cs.CV},
title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
year = {2015}
}
@inproceedings{attentionisallyouneed,
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Guyon, I and Luxburg, U V and Bengio, S and Wallach, H and Fergus, R and Vishwanathan, S and Garnett, R},
publisher = {Curran Associates, Inc.},
title = {Attention is All you Need},
volume = {30},
year = {2017}
}
@incollection{NIPS2011_4307,
author = {Girshick, Ross B and Felzenszwalb, Pedro F and McAllester, David A},
booktitle = {Advances in Neural Information Processing Systems 24},
editor = {Shawe-Taylor, J and Zemel, R S and Bartlett, P L and Pereira, F and Weinberger, K Q},
pages = {442--450},
publisher = {Curran Associates, Inc.},
title = {Object Detection with Grammar Models},
year = {2011}
}
@inproceedings{liu2017priornetworks,
author = {Liu, Y and Jin, L},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.368},
pages = {3454--3461},
title = {Deep Matching Prior Network: Toward Tighter Multi-oriented Text Detection},
year = {2017}
}
@article{trainable-system-od,
author = {Papageorgiou, Constantine and Poggio, Tomaso},
doi = {10.1023/A:1008162616689},
journal = {International Journal of Computer Vision},
pages = {15--33},
title = {A Trainable System for Object Detection},
volume = {38},
year = {2000}
}
@inproceedings{detr,
abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
address = {Cham},
author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
booktitle = {Computer Vision -- ECCV 2020},
editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
isbn = {978-3-030-58452-8},
pages = {213--229},
publisher = {Springer International Publishing},
title = {End-to-End Object Detection with Transformers},
year = {2020}
}
@inproceedings{rfcn,
address = {Red Hook, NY, USA},
author = {Dai, Jifeng and Li, Yi and He, Kaiming and Sun, Jian},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
isbn = {9781510838819},
pages = {379--387},
publisher = {Curran Associates Inc.},
series = {NIPS'16},
title = {R-FCN: Object Detection via Region-Based Fully Convolutional Networks},
year = {2016}
}
@inproceedings{tang2020karnet,
author = {Tianhang, Tang and Yiguang, Liu and Yunan, Zheng and Xianzhen, Zhu and Zhao, Yangyu},
booktitle = {International Conference on Computer Intelligent Systems and Network Remote Control (CISNRC 2020)},
doi = {10.12783/dtcse/cisnr2020/35158},
title = {Rotating Objects Detection in Aerial Images via Attention Denoising and Angle Loss Refining},
year = {2020}
}
@inproceedings{zfnet,
abstract = {Large Convolutional Network models have recently
demonstrated impressive classification performance on the
ImageNet benchmark Krizhevsky et al. [18]. However there is no
clear understanding of why they perform so well, or how they
might be improved. In this paper we explore both issues. We
introduce a novel visualization technique that gives insight
into the function of intermediate feature layers and the
operation of the classifier. Used in a diagnostic role, these
visualizations allow us to find model architectures that
outperform Krizhevsky et al on the ImageNet classification
benchmark. We also perform an ablation study to discover the
performance contribution from different model layers. We show
our ImageNet model generalizes well to other datasets: when
the softmax classifier is retrained, it convincingly beats the
current state-of-the-art results on Caltech-101 and
Caltech-256 datasets.},
address = {Cham},
author = {Zeiler, Matthew D and Fergus, Rob},
booktitle = {Computer Vision -- ECCV 2014},
editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
isbn = {978-3-319-10590-1},
pages = {818--833},
publisher = {Springer International Publishing},
title = {Visualizing and Understanding Convolutional Networks},
year = {2014}
}
@article{Chollet2017XceptionDL,
author = {Chollet, Fran{\c{c}}ois},
journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {1800--1807},
title = {Xception: Deep Learning with Depthwise Separable Convolutions},
year = {2017}
}
@article{Felzenszwalb2008ADT,
author = {Felzenszwalb, Pedro F and McAllester, David A and Ramanan, Deva},
journal = {2008 IEEE Conference on Computer Vision and Pattern Recognition},
pages = {1--8},
title = {A discriminatively trained, multiscale, deformable part model},
year = {2008}
}
@inproceedings{ioffe2015batchnorm,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82{\%} top-5 test error, exceeding the accuracy of human raters.},
address = {Lille, France},
author = {Ioffe, Sergey and Szegedy, Christian},
booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
editor = {Bach, Francis and Blei, David},
pages = {448--456},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
volume = {37},
year = {2015}
}
@article{Papadopoulos2017ExtremeCF,
author = {Papadopoulos, Dim P and Uijlings, Jasper R R and Keller, Frank and Ferrari, Vittorio},
journal = {2017 IEEE International Conference on Computer Vision (ICCV)},
pages = {4940--4949},
title = {Extreme Clicking for Efficient Object Annotation},
year = {2017}
}
@misc{wu2019detectron2,
author = {Wu, Yuxin and Kirillov, Alexander and Massa, Francisco and Lo, Wan-Yen and Girshick, Ross},
howpublished = {$\backslash$url{\{}https://github.com/facebookresearch/detectron2{\}}},
title = {Detectron2},
year = {2019}
}
@inproceedings{krogh1991weightdecay,
abstract = {It has been observed in numerical simulations that a weight
decay can improve generalization in a feed-forward neural
network. This paper explains why. It is proven that a weight
decay has two effects in a linear network. First, it
suppresses any irrelevant components of the weight vector by
choosing the smallest vector that solves the learning problem.
Second, if the size is chosen right, a weight decay can
suppress some of the effects of static noise on the targets,
which improves generalization quite a lot. It is then shown
how to extend these results to networks with hidden layers and
non-linear units. Finally the theory is confirmed by some
numerical simulations using the data from NetTalk.},
address = {San Francisco, CA, USA},
author = {Krogh, Anders and Hertz, John A},
booktitle = {Proceedings of the 4th International Conference on Neural Information Processing Systems},
isbn = {1558602224},
pages = {950--957},
publisher = {Morgan Kaufmann Publishers Inc.},
series = {NIPS'91},
title = {A Simple Weight Decay Can Improve Generalization},
year = {1991}
}
@misc{dpm-v5,
author = {Girshick, R B and Felzenszwalb, P F and McAllester, D},
howpublished = {http://people.cs.uchicago.edu/{\~{}}rbg/latent-release5/},
title = {Discriminatively Trained Deformable Part Models, Release 5}
}
@article{sift,
author = {Lowe, David G},
journal = {International journal of computer vision},
number = {2},
pages = {91--110},
publisher = {Springer},
title = {Distinctive image features from scale-invariant keypoints},
volume = {60},
year = {2004}
}
@misc{huang2015densebox,
archivePrefix = {arXiv},
arxivId = {cs.CV/1509.04874},
author = {Huang, Lichao and Yang, Yi and Deng, Yafeng and Yu, Yinan},
eprint = {1509.04874},
primaryClass = {cs.CV},
title = {DenseBox: Unifying Landmark Localization with End to End Object Detection},
year = {2015}
}
@inproceedings{wang2018pelee,
address = {Red Hook, NY, USA},
author = {Wang, Robert J and Li, Xiang and Ling, Charles X},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {1967--1976},
publisher = {Curran Associates Inc.},
series = {NIPS'18},
title = {Pelee: A Real-Time Object Detection System on Mobile Devices},
year = {2018}
}
@incollection{pytorch,
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {Wallach, H and Larochelle, H and Beygelzimer, A and d$\backslash$textquotesingle Alch{\'{e}}-Buc, F and Fox, E and Garnett, R},
pages = {8024--8035},
publisher = {Curran Associates, Inc.},
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
year = {2019}
}
@inproceedings{resnext,
author = {Xie, S and Girshick, R and Doll{\'{a}}r, P and Tu, Z and He, K},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {5987--5995},
title = {Aggregated Residual Transformations for Deep Neural Networks},
year = {2017}
}
@misc{tdm,
archivePrefix = {arXiv},
arxivId = {cs.CV/1612.06851},
author = {Shrivastava, Abhinav and Sukthankar, Rahul and Malik, Jitendra and Gupta, Abhinav},
eprint = {1612.06851},
primaryClass = {cs.CV},
title = {Beyond Skip Connections: Top-Down Modulation for Object Detection},
year = {2017}
}
@inproceedings{he2015weightinit,
author = {He, K and Zhang, X and Ren, S and Sun, J},
booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.123},
pages = {1026--1034},
title = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},
year = {2015}
}
@inproceedings{chen2019detnas,
author = {Chen, Yukang and Yang, Tong and Zhang, Xiangyu and MENG, GAOFENG and Xiao, Xinyu and Sun, Jian},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Wallach, H and Larochelle, H and Beygelzimer, A and d$\backslash$textquotesingle Alch{\'{e}}-Buc, F and Fox, E and Garnett, R},
publisher = {Curran Associates, Inc.},
title = {DetNAS: Backbone Search for Object Detection},
volume = {32},
year = {2019}
}
@article{cnn-rebirth,
address = {New York, NY, USA},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
doi = {10.1145/3065386},
issn = {0001-0782},
journal = {Commun. ACM},
month = {may},
number = {6},
pages = {84--90},
publisher = {Association for Computing Machinery},
title = {ImageNet Classification with Deep Convolutional Neural Networks},
volume = {60},
year = {2017}
}
@misc{howard2017mobilenet,
archivePrefix = {arXiv},
arxivId = {cs.CV/1704.04861},
author = {Howard, Andrew G and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
eprint = {1704.04861},
primaryClass = {cs.CV},
title = {MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications},
year = {2017}
}
@article{dior-data,
author = {Li, Ke and Wan, Gang and Cheng, Gong and Meng, Liqiu and Han, Junwei},
doi = {10.1016/j.isprsjprs.2019.11.023},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
pages = {296--307},
title = {Object detection in optical remote sensing images: A survey and a new benchmark},
volume = {159},
year = {2020}
}
@inproceedings{icdar15,
address = {USA},
author = {Karatzas, Dimosthenis and Gomez-Bigorda, Lluis and Nicolaou, Anguelos and Ghosh, Suman and Bagdanov, Andrew and Iwamura, Masakazu and Matas, Jiri and Neumann, Lukas and Chandrasekhar, Vijay Ramaseshan and Lu, Shijian and Shafait, Faisal and Uchida, Seiichi and Valveny, Ernest},
booktitle = {Proceedings of the 2015 13th International Conference on Document Analysis and Recognition (ICDAR)},
doi = {10.1109/ICDAR.2015.7333942},
isbn = {9781479918058},
pages = {1156--1160},
publisher = {IEEE Computer Society},
series = {ICDAR '15},
title = {ICDAR 2015 Competition on Robust Reading},
year = {2015}
}
@inproceedings{fast-rcnn,
author = {Girshick, R},
booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
pages = {1440--1448},
title = {Fast R-CNN},
year = {2015}
}
@inproceedings{rcnn,
author = {Girshick, R and Donahue, J and Darrell, T and Malik, J},
booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
pages = {580--587},
title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
year = {2014}
}
@article{example-based-od,
address = {USA},
author = {Mohan, Anuj and Papageorgiou, Constantine and Poggio, Tomaso},
doi = {10.1109/34.917571},
issn = {0162-8828},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
keywords = {Object detection,components.,machine learning,pattern recognition,people detection},
month = {apr},
number = {4},
pages = {349--361},
publisher = {IEEE Computer Society},
title = {Example-Based Object Detection in Images by Components},
volume = {23},
year = {2001}
}
@inproceedings{Viola01rapidobject,
author = {Viola, Paul and Jones, Michael},
booktitle = {CVPR},
title = {Rapid object detection using a boosted cascade of simple features},
year = {2001}
}
@phdthesis{10.5555/2520924,
address = {USA},
author = {Girshick, Ross Brook},
isbn = {9781267437594},
publisher = {University of Chicago},
title = {From Rigid Templates to Grammars: Object Detection with Structured Models},
year = {2012}
}
@inproceedings{rahman2011gabor,
author = {Rahman, M H and Pickering, M R and Frater, M R},
booktitle = {2011 International Conference on Digital Image Computing: Techniques and Applications},
doi = {10.1109/DICTA.2011.107},
pages = {602--607},
title = {Scale and Rotation Invariant Gabor Features for Texture Retrieval},
year = {2011}
}
@misc{fu2017dssd,
archivePrefix = {arXiv},
arxivId = {cs.CV/1701.06659},
author = {Fu, Cheng-Yang and Liu, Wei and Ranga, Ananth and Tyagi, Ambrish and Berg, Alexander C},
eprint = {1701.06659},
primaryClass = {cs.CV},
title = {DSSD : Deconvolutional Single Shot Detector},
year = {2017}
}
@inproceedings{nn-calib,
author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {1321--1330},
publisher = {JMLR.org},
series = {ICML'17},
title = {On Calibration of Modern Neural Networks},
year = {2017}
}
@inproceedings{mscoco,
abstract = {We present a new dataset with the goal of advancing the
state-of-the-art in object recognition by placing the question
of object recognition in the context of the broader question
of scene understanding. This is achieved by gathering images
of complex everyday scenes containing common objects in their
natural context. Objects are labeled using per-instance
segmentations to aid in precise object localization. Our
dataset contains photos of 91 objects types that would be
easily recognizable by a 4 year old. With a total of 2.5
million labeled instances in 328k images, the creation of our
dataset drew upon extensive crowd worker involvement via novel
user interfaces for category detection, instance spotting and
instance segmentation. We present a detailed statistical
analysis of the dataset in comparison to PASCAL, ImageNet, and
SUN. Finally, we provide baseline performance analysis for
bounding box and segmentation detection results using a
Deformable Parts Model.},
address = {Cham},
author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'{a}}r, Piotr and Zitnick, C Lawrence},
booktitle = {Computer Vision -- ECCV 2014},
editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
isbn = {978-3-319-10602-1},
pages = {740--755},
publisher = {Springer International Publishing},
title = {Microsoft COCO: Common Objects in Context},
year = {2014}
}
@inproceedings{cohen2016groupeqcnn,
abstract = {We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of layer that enjoys a substantially higher degree of weight sharing than regular convolution layers. G-convolutions increase the expressive capacity of the network without increasing the number of parameters. Group convolution layers are easy to use and can be implemented with negligible computational overhead for discrete groups generated by translations, reflections and rotations. G-CNNs achieve state of the art results on CIFAR10 and rotated MNIST.},
address = {New York, New York, USA},
author = {Cohen, Taco and Welling, Max},
booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
editor = {Balcan, Maria Florina and Weinberger, Kilian Q},
pages = {2990--2999},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {Group Equivariant Convolutional Networks},
volume = {48},
year = {2016}
}
@inproceedings{cohen2019gaugeeqcnn,
abstract = {The principle of equivariance to symmetry transformations enables a theoretically grounded approach to neural network architecture design. Equivariant networks have shown excellent performance and data efficiency on vision and medical imaging problems that exhibit symmetries. Here we show how this principle can be extended beyond global symmetries to local gauge transformations. This enables the development of a very general class of convolutional neural networks on manifolds that depend only on the intrinsic geometry,
and which includes many popular methods from equivariant and geometric deep learning. We implement gauge equivariant CNNs for signals defined on the surface of the icosahedron,
which provides a reasonable approximation of the sphere. By choosing to work with this very regular manifold,
we are able to implement the gauge equivariant convolution using a single conv2d call,
making it a highly scalable and practical alternative to Spherical CNNs. Using this method,
we demonstrate substantial improvements over previous methods on the task of segmenting omnidirectional images and global climate patterns.},
author = {Cohen, Taco and Weiler, Maurice and Kicanaoglu, Berkay and Welling, Max},
booktitle = {Proceedings of the 36th International Conference on Machine Learning},
editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
pages = {1321--1330},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {Gauge Equivariant Convolutional Networks and the Icosahedral {\{}CNN{\}}},
volume = {97},
year = {2019}
}
@inproceedings{haley1995gabor,
author = {Haley, G M and Manjunath, B S},
booktitle = {Proceedings., International Conference on Image Processing},
doi = {10.1109/ICIP.1995.529696},
pages = {262--265 vol.1},
title = {Rotation-invariant texture classification using modified Gabor filters},
volume = {1},
year = {1995}
}
@inproceedings{gulrajani1027gradientpenalty,
author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Guyon, I and Luxburg, U V and Bengio, S and Wallach, H and Fergus, R and Vishwanathan, S and Garnett, R},
pages = {5767--5777},
publisher = {Curran Associates, Inc.},
title = {Improved Training of Wasserstein GANs},
volume = {30},
year = {2017}
}
@inproceedings{huang2017densenet,
author = {Huang, G and Liu, Z and {Van Der Maaten}, L and Weinberger, K Q},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.243},
pages = {2261--2269},
title = {Densely Connected Convolutional Networks},
year = {2017}
}
@inproceedings{zhou2018scale,
author = {Zhou, P and Ni, B and Geng, C and Hu, J and Xu, Y},
booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00062},
pages = {528--537},
title = {Scale-Transferrable Object Detection},
year = {2018}
}
@article{od-disc,
author = {Felzenszwalb, P F and Girshick, R B and McAllester, D and Ramanan, D},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {9},
pages = {1627--1645},
title = {Object Detection with Discriminatively Trained Part-Based Models},
volume = {32},
year = {2010}
}
@inproceedings{nfs,
author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
pages = {1530--1538},
publisher = {JMLR.org},
series = {ICML'15},
title = {Variational Inference with Normalizing Flows},
year = {2015}
}
@inproceedings{selective-search,
author = {van de Sande, K E A and Uijlings, J R R and Gevers, T and Smeulders, A W M},
booktitle = {2011 International Conference on Computer Vision},
pages = {1879--1886},
title = {Segmentation as selective search for object recognition},
year = {2011}
}
@inproceedings{ens-exem-svms,
author = {Malisiewicz, T and Gupta, A and Efros, A A},
booktitle = {2011 International Conference on Computer Vision},
pages = {89--96},
title = {Ensemble of exemplar-SVMs for object detection and beyond},
year = {2011}
}
@inproceedings{weiler2019e2cnn,
author = {Weiler, Maurice and Cesa, Gabriele},
booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
title = {General E(2)-Equivariant Steerable CNNs},
year = {2019}
}
@inproceedings{sppnet,
abstract = {Existing deep convolutional neural networks (CNNs) require
a fixed-size (e.g.Â 224{\{}$\backslash$texttimes{\}}224) input image. This
requirement is ``artificial'' and may hurt the recognition
accuracy for the images or sub-images of an arbitrary
size/scale. In this work, we equip the networks with a more
principled pooling strategy, ``spatial pyramid pooling'', to
eliminate the above requirement. The new network structure,
called SPP-net, can generate a fixed-length representation
regardless of image size/scale. By removing the fixed-size
limitation, we can improve all CNN-based image classification
methods in general. Our SPP-net achieves state-of-the-art
accuracy on the datasets of ImageNet 2012, Pascal VOC 2007,
and Caltech101.},
address = {Cham},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Computer Vision -- ECCV 2014},
editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
isbn = {978-3-319-10578-9},
pages = {346--361},
publisher = {Springer International Publishing},
title = {Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition},
year = {2014}
}
@inproceedings{icdar17,
author = {Forn{\'{e}}s, A and Romero, V and Bar{\'{o}}, A and Toledo, J I and S{\'{a}}nchez, J A and Vidal, E and Llad{\'{o}}s, J},
booktitle = {2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)},
pages = {1389--1394},
title = {ICDAR2017 Competition on Information Extraction in Historical Handwritten Records},
volume = {01},
year = {2017}
}
@article{OpenImages,
author = {Kuznetsova, Alina and Rom, Hassan and Alldrin, Neil and Uijlings, Jasper and Krasin, Ivan and Pont-Tuset, Jordi and Kamali, Shahab and Popov, Stefan and Malloci, Matteo and Kolesnikov, Alexander and Duerig, Tom and Ferrari, Vittorio},
journal = {IJCV},
title = {The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale},
year = {2020}
}
@inproceedings{od-framework,
author = {Papageorgiou, C P and Oren, M and Poggio, T},
booktitle = {Sixth International Conference on Computer Vision (IEEE Cat. No. 98CH36271)},
pages = {555--562},
title = {A general framework for object detection},
year = {1998}
}
@techreport{fddbTech,
author = {Jain, Vidit and Learned-Miller, Erik},
institution = {University of Massachusetts, Amherst},
number = {UM-CS-2010-009},
title = {FDDB: A Benchmark for Face Detection in Unconstrained Settings},
year = {2010}
}
@inproceedings{ohem,
author = {Shrivastava, A and Gupta, A and Girshick, R},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {761--769},
title = {Training Region-Based Object Detectors with Online Hard Example Mining},
year = {2016}
}
@inproceedings{4587586,
author = {Lampert, C H and Blaschko, M B and Hofmann, T},
booktitle = {2008 IEEE Conference on Computer Vision and Pattern Recognition},
pages = {1--8},
title = {Beyond sliding windows: Object localization by efficient subwindow search},
year = {2008}
}
@article{pascal-voc,
address = {USA},
author = {Everingham, Mark and Gool, Luc and Williams, Christopher K and Winn, John and Zisserman, Andrew},
doi = {10.1007/s11263-009-0275-4},
issn = {0920-5691},
journal = {Int. J. Comput. Vision},
keywords = {Benchmark,Database,Object detection,Object recognition},
month = {jun},
number = {2},
pages = {303--338},
publisher = {Kluwer Academic Publishers},
title = {The Pascal Visual Object Classes (VOC) Challenge},
volume = {88},
year = {2010}
}
@misc{Li2017LightHeadRI,
archivePrefix = {arXiv},
arxivId = {cs.CV/1711.07264},
author = {Li, Zeming and Peng, Chao and Yu, Gang and Zhang, Xiangyu and Deng, Yangdong and Sun, Jian},
eprint = {1711.07264},
primaryClass = {cs.CV},
title = {Light-Head R-CNN: In Defense of Two-Stage Object Detector},
year = {2017}
}
@misc{law2019cornernetlite,
archivePrefix = {arXiv},
arxivId = {cs.CV/1904.08900},
author = {Law, Hei and Teng, Yun and Russakovsky, Olga and Deng, Jia},
eprint = {1904.08900},
primaryClass = {cs.CV},
title = {CornerNet-Lite: Efficient Keypoint Based Object Detection},
year = {2019}
}
@inproceedings{Poon2011,
author = {Poon, H and Domingos, P},
booktitle = {Proceedings of UAI},
pages = {337--346},
title = {Sum-Product Networks: A New Deep Architecture},
year = {2011}
}
@inproceedings{cohen2017steerablecnns,
author = {Cohen, Taco S and Welling, Max},
booktitle = {5th International Conference on Learning Representations, {\{}ICLR{\}} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
publisher = {OpenReview.net},
title = {Steerable CNNs},
year = {2017}
}
@inproceedings{hog-features,
author = {Dalal, N and Triggs, B},
booktitle = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
pages = {886--893 vol. 1},
title = {Histograms of oriented gradients for human detection},
volume = {1},
year = {2005}
}
@inproceedings{cohen2019eqcnntheory,
author = {Cohen, Taco S and Geiger, Mario and Weiler, Maurice},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Wallach, H and Larochelle, H and Beygelzimer, A and d$\backslash$textquotesingle Alch{\'{e}}-Buc, F and Fox, E and Garnett, R},
publisher = {Curran Associates, Inc.},
title = {A General Theory of Equivariant CNNs on Homogeneous Spaces},
volume = {32},
year = {2019}
}
@incollection{NIPS2017_6822,
author = {Newell, Alejandro and Huang, Zhiao and Deng, Jia},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {Guyon, I and Luxburg, U V and Bengio, S and Wallach, H and Fergus, R and Vishwanathan, S and Garnett, R},
pages = {2277--2287},
publisher = {Curran Associates, Inc.},
title = {Associative Embedding: End-to-End Learning for Joint Detection and Grouping},
year = {2017}
}
@misc{cohen2018IntertwinersBI,
archivePrefix = {arXiv},
arxivId = {cs.LG/1803.10743},
author = {Cohen, Taco S and Geiger, Mario and Weiler, Maurice},
eprint = {1803.10743},
primaryClass = {cs.LG},
title = {Intertwiners between Induced Representations (with Applications to the Theory of Equivariant Neural Networks)},
year = {2018}
}
@inproceedings{weiler2018learnsteerableroteqcnn,
author = {Weiler, M and Hamprecht, F A and Storath, M},
booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00095},
pages = {849--858},
title = {Learning Steerable Filters for Rotation Equivariant CNNs},
year = {2018}
}
@inproceedings{5539906,
author = {Felzenszwalb, P F and Girshick, R B and McAllester, D},
booktitle = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {2241--2248},
title = {Cascade object detection with deformable part models},
year = {2010}
}
@inproceedings{weiler2018steerablecnns3d,
author = {Weiler, Maurice and Geiger, Mario and Welling, Max and Boomsma, Wouter and Cohen, Taco S},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Bengio, S and Wallach, H and Larochelle, H and Grauman, K and Cesa-Bianchi, N and Garnett, R},
publisher = {Curran Associates, Inc.},
title = {3D Steerable CNNs: Learning Rotationally Equivariant Features in Volumetric Data},
volume = {31},
year = {2018}
}
@inproceedings{shen2017dsod,
author = {Shen, Z and Liu, Z and Li, J and Jiang, Y and Chen, Y and Xue, X},
booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2017.212},
pages = {1937--1945},
title = {DSOD: Learning Deeply Supervised Object Detectors from Scratch},
year = {2017}
}
